# 実装サンプル

**目的**: 各コンポーネントの実装例とサンプルコードを提供

**注意**: これらは実装の参考例です。実際の実装時には設計書（01, 02, 03）を優先してください。

---

## 6. テストコード

### 6.1 OllamaClient テスト完全版

```python
# tests/test_ollama_client.py

import pytest
import time
from core.ollama_client import OllamaClient

class TestOllamaClient:
    """OllamaClient ユニットテスト（TDD方式）"""
    
    def test_init(self):
        """TC-O-001: 初期化テスト"""
        client = OllamaClient()
        assert client.base_url == "http://localhost:11434/v1"
        assert client.model == "gemma3:12b"
        assert client.timeout == 30.0
        assert client.max_retries == 3
    
    def test_is_healthy(self):
        """TC-O-002: Ollama接続確認"""
        client = OllamaClient()
        assert client.is_healthy() == True
    
    def test_generate_basic(self):
        """TC-O-003: 基本的なテキスト生成"""
        client = OllamaClient()
        messages = [{"role": "user", "content": "こんにちは"}]
        response = client.generate(messages)
        
        assert isinstance(response, str)
        assert len(response) > 0
        assert len(response) < 5000  # 異常な長さでない
    
    def test_generate_with_params(self):
        """TC-O-004: パラメータ付き生成"""
        client = OllamaClient()
        messages = [{"role": "user", "content": "こんにちは"}]
        response = client.generate(
            messages, 
            temperature=0.5, 
            max_tokens=100
        )
        
        assert isinstance(response, str)
        assert len(response) > 0
    
    def test_embed_basic(self):
        """TC-O-005: 基本的な埋め込み生成"""
        client = OllamaClient()
        embedding = client.embed("JetRacerとは自律走行車です")
        
        assert isinstance(embedding, list)
        assert len(embedding) == 1024  # mxbai-embed-large
        assert all(isinstance(x, float) for x in embedding)
    
    def test_retry_mechanism(self):
        """TC-O-006: リトライ機構テスト"""
        # 無効なURLで接続失敗をシミュレート
        client = OllamaClient(
            base_url="http://invalid:11434/v1",
            max_retries=3
        )
        
        start_time = time.time()
        with pytest.raises(Exception):
            client.generate([{"role": "user", "content": "test"}])
        elapsed = time.time() - start_time
        
        # exponential backoff: 2^0 + 2^1 + 2^2 = 1 + 2 + 4 = 7秒以上
        assert elapsed > 7
    
    def test_timeout(self):
        """TC-O-007: タイムアウト処理"""
        client = OllamaClient(timeout=1.0)  # 1秒でタイムアウト
        
        # 非常に長い応答を要求
        messages = [{"role": "user", "content": "長い物語を書いて" * 100}]
        
        with pytest.raises(Exception):
            client.generate(messages, max_tokens=10000)
```

### 6.2 RAGEngine テスト完全版

```python
# tests/test_rag_engine.py

import pytest
import shutil
import os
from core.ollama_client import OllamaClient
from core.rag_engine import RAGEngine

@pytest.fixture
def rag_engine():
    """テスト用RAGEngine（各テストで独立）"""
    test_chroma_path = "./data/test_chroma_db"
    
    # クリーンアップ
    if os.path.exists(test_chroma_path):
        shutil.rmtree(test_chroma_path)
    
    client = OllamaClient()
    rag = RAGEngine(client, chroma_path=test_chroma_path)
    
    yield rag
    
    # テスト後クリーンアップ
    if os.path.exists(test_chroma_path):
        shutil.rmtree(test_chroma_path)

class TestRAGEngine:
    """RAGEngine ユニットテスト（TDD方式）"""
    
    def test_init(self, rag_engine):
        """TC-R-001: ChromaDB初期化"""
        assert rag_engine.collection is not None
        assert rag_engine.collection.count() == 0
    
    def test_add_knowledge(self, rag_engine):
        """TC-R-002: 知識追加"""
        texts = ["JetRacerは自律走行車です", "センサーで周囲を認識します"]
        metadatas = [
            {"domain": "technical", "character": "both"},
            {"domain": "technical", "character": "both"}
        ]
        
        rag_engine.add_knowledge(texts, metadatas)
        assert rag_engine.collection.count() == 2
    
    def test_search_basic(self, rag_engine):
        """TC-R-003: 基本検索"""
        # 知識追加
        texts = ["JetRacerは自律走行車です", "モーター制御はPWM信号です"]
        metadatas = [{"domain": "technical", "character": "both"}] * 2
        rag_engine.add_knowledge(texts, metadatas)
        
        # 検索
        results = rag_engine.search("JetRacerとは", top_k=1)
        
        assert len(results) == 1
        assert "JetRacer" in results[0]["text"]
        assert "score" in results[0]
        assert results[0]["score"] > 0.5
    
    def test_search_with_filter(self, rag_engine):
        """TC-R-004: フィルタ検索"""
        # 異なるメタデータで追加
        texts = ["やなは直感的", "あゆは分析的"]
        metadatas = [
            {"domain": "character", "character": "yana"},
            {"domain": "character", "character": "ayu"}
        ]
        rag_engine.add_knowledge(texts, metadatas)
        
        # やな限定で検索
        results = rag_engine.search(
            "性格",
            top_k=10,
            filters={"character": "yana"}
        )
        
        assert len(results) == 1
        assert "やな" in results[0]["text"]
    
    def test_search_accuracy(self, rag_engine):
        """TC-R-005: 検索精度"""
        # 明確に異なる知識を追加
        texts = [
            "JetRacerは自律走行車です",
            "猫は可愛い動物です",
            "太陽は恒星です"
        ]
        metadatas = [{"domain": "technical", "character": "both"}] * 3
        rag_engine.add_knowledge(texts, metadatas)
        
        # JetRacerに関する検索
        results = rag_engine.search("自律走行車について", top_k=1)
        
        # 最も関連度が高いのはJetRacerの知識
        assert "JetRacer" in results[0]["text"]
        assert results[0]["score"] > 0.7
    
    def test_chunk_text(self, rag_engine):
        """TC-R-006: テキスト分割"""
        long_text = "A" * 2500  # 2500文字
        chunks = rag_engine._chunk_text(long_text, max_chars=1000)
        
        assert len(chunks) == 3  # 1000 + 1000 + 500
        assert all(len(chunk) <= 1000 for chunk in chunks)
    
    def test_init_from_files(self, rag_engine):
        """TC-R-007: ファイルからの初期化"""
        metadata_mapping = {
            "jetracer_tech.txt": {
                "domain": "technical",
                "character": "both"
            }
        }
        
        rag_engine.init_from_files("./knowledge", metadata_mapping)
        assert rag_engine.collection.count() > 0
    
    def test_empty_search(self, rag_engine):
        """TC-R-008: 空のデータベースで検索"""
        results = rag_engine.search("何か", top_k=3)
        assert len(results) == 0
```

### 6.3 Character テスト完全版

```python
# tests/test_character.py

import pytest
import shutil
import os
from core.ollama_client import OllamaClient
from core.rag_engine import RAGEngine
from core.character import Character

@pytest.fixture
def test_character():
    """テスト用Character"""
    test_path = "./data/test_char_db"
    if os.path.exists(test_path):
        shutil.rmtree(test_path)
    
    client = OllamaClient()
    rag = RAGEngine(client, chroma_path=test_path)
    
    # 簡易知識
    rag.add_knowledge(
        ["JetRacerは自律走行車です"],
        [{"domain": "technical", "character": "both"}]
    )
    
    char = Character("yana", "./personas/yana.yaml", client, rag)
    
    yield char
    
    if os.path.exists(test_path):
        shutil.rmtree(test_path)

class TestCharacter:
    """Character ユニットテスト（TDD方式）"""
    
    def test_init(self, test_character):
        """TC-C-001: 初期化テスト"""
        assert test_character.name == "yana"
        assert test_character.config is not None
        assert "system_prompt" in test_character.config
        assert len(test_character.history) == 0
    
    def test_respond_basic(self, test_character):
        """TC-C-002: 基本応答"""
        response = test_character.respond("こんにちは", use_rag=False)
        
        assert isinstance(response, str)
        assert len(response) > 0
    
    def test_respond_with_rag(self, test_character):
        """TC-C-003: RAG統合"""
        response = test_character.respond("JetRacerって何？", use_rag=True)
        
        assert len(response) > 0
        # JetRacerに言及しているか
        assert any(keyword in response for keyword in 
                   ["JetRacer", "自律", "走行", "車"])
    
    def test_history_management(self, test_character):
        """TC-C-004: 履歴管理"""
        test_character.respond("こんにちは")
        assert len(test_character.history) == 2  # user + assistant
        
        test_character.respond("元気？")
        assert len(test_character.history) == 4
    
    def test_clear_history(self, test_character):
        """TC-C-005: 履歴クリア"""
        test_character.respond("こんにちは")
        test_character.respond("元気？")
        assert len(test_character.history) == 4
        
        test_character.clear_history()
        assert len(test_character.history) == 0
    
    def test_query_rewrite(self, test_character):
        """TC-C-006: クエリ書き換え"""
        # 最初の質問
        test_character.respond("JetRacerって何？", use_rag=False)
        
        # 代名詞を使った質問
        vague_query = "それって速いの？"
        rewritten = test_character._rewrite_query(vague_query)
        
        # 「JetRacer」が含まれるか
        assert any(keyword in rewritten for keyword in 
                   ["JetRacer", "自律", "走行"])
    
    def test_max_history_limit(self, test_character):
        """TC-C-007: 履歴上限テスト"""
        # 15ターン会話（30メッセージ）
        for i in range(15):
            test_character.respond(f"質問{i}")
        
        # 最大10ターン（20メッセージ）に制限
        assert len(test_character.history) == 20
```

### 6.4 統合テスト完全版

```python
# tests/test_integration.py

import pytest
import yaml
import shutil
import os
from core.ollama_client import OllamaClient
from core.rag_engine import RAGEngine
from core.character import Character

@pytest.fixture
def system_setup():
    """システム全体セットアップ"""
    test_path = "./data/test_integration_db"
    if os.path.exists(test_path):
        shutil.rmtree(test_path)
    
    # 設定読み込み
    with open("config.yaml", "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    
    # 初期化
    client = OllamaClient(
        base_url=config["ollama"]["base_url"],
        model=config["ollama"]["llm_model"]
    )
    
    rag = RAGEngine(
        ollama_client=client,
        chroma_path=test_path
    )
    
    # 知識投入
    metadata_mapping = {
        item["file"]: item["metadata"]
        for item in config["knowledge"]["sources"]
    }
    rag.init_from_files(
        config["knowledge"]["source_dir"],
        metadata_mapping
    )
    
    # キャラクター
    yana = Character("yana", "./personas/yana.yaml", client, rag)
    ayu = Character("ayu", "./personas/ayu.yaml", client, rag)
    
    yield {"yana": yana, "ayu": ayu, "config": config}
    
    if os.path.exists(test_path):
        shutil.rmtree(test_path)

class TestIntegration:
    """統合テスト（TDD方式）"""
    
    def test_full_flow(self, system_setup):
        """TC-I-001: 全体フロー"""
        yana = system_setup["yana"]
        
        response = yana.respond("JetRacerって何？")
        assert len(response) > 0
    
    def test_character_switch(self, system_setup):
        """TC-I-002: キャラクター切替"""
        yana = system_setup["yana"]
        ayu = system_setup["ayu"]
        
        yana_response = yana.respond("こんにちは")
        ayu_response = ayu.respond("こんにちは")
        
        # 両方とも応答がある
        assert len(yana_response) > 0
        assert len(ayu_response) > 0
        
        # 口調が違う（手動確認推奨）
        print(f"やな: {yana_response}")
        print(f"あゆ: {ayu_response}")
    
    def test_multi_turn(self, system_setup):
        """TC-I-003: 複数ターン会話"""
        yana = system_setup["yana"]
        
        responses = []
        questions = [
            "こんにちは",
            "JetRacerって何？",
            "センサーはどうなってる？",
            "速度はどのくらい？",
            "ありがとう"
        ]
        
        for q in questions:
            r = yana.respond(q)
            responses.append(r)
        
        # 5ターン全て応答がある
        assert len(responses) == 5
        assert all(len(r) > 0 for r in responses)
    
    def test_rag_context(self, system_setup):
        """TC-I-004: RAG文脈参照"""
        yana = system_setup["yana"]
        
        # 技術的な質問
        response = yana.respond("JetRacerのセンサーについて詳しく教えて")
        
        # 知識ベースの情報が含まれているか
        assert any(keyword in response for keyword in [
            "センサー", "超音波", "IMU", "カメラ", "測定"
        ])
    
    def test_config_loading(self, system_setup):
        """TC-I-005: 設定読込"""
        config = system_setup["config"]
        
        assert "ollama" in config
        assert "rag" in config
        assert "characters" in config
```

### 6.5 パフォーマンステスト完全版

```python
# tests/test_performance.py

import pytest
import time
from core.ollama_client import OllamaClient
from core.rag_engine import RAGEngine
from core.character import Character

class TestPerformance:
    """パフォーマンステスト"""
    
    def test_response_time(self):
        """TC-P-001: 応答時間テスト"""
        client = OllamaClient()
        rag = RAGEngine(client, chroma_path="./data/test_perf_db")
        char = Character("yana", "./personas/yana.yaml", client, rag)
        
        start = time.time()
        response = char.respond("こんにちは", use_rag=False)
        elapsed = time.time() - start
        
        print(f"\n応答時間: {elapsed:.2f}秒")
        assert elapsed < 5.0  # 5秒以内
    
    def test_rag_search_time(self):
        """TC-P-002: RAG検索時間"""
        client = OllamaClient()
        rag = RAGEngine(client, chroma_path="./data/test_perf_db")
        
        # 大量の知識追加
        texts = [f"テスト文章{i}" * 10 for i in range(100)]
        metadatas = [{"domain": "test"}] * 100
        rag.add_knowledge(texts, metadatas)
        
        start = time.time()
        results = rag.search("テスト", top_k=3)
        elapsed = time.time() - start
        
        print(f"\n検索時間: {elapsed:.2f}秒")
        assert elapsed < 0.5  # 0.5秒以内
    
    def test_embedding_time(self):
        """TC-P-003: 埋め込み時間"""
        client = OllamaClient()
        
        start = time.time()
        embedding = client.embed("JetRacerは自律走行車です" * 10)
        elapsed = time.time() - start
        
        print(f"\n埋め込み時間: {elapsed:.2f}秒")
        assert elapsed < 0.2  # 0.2秒以内
    
    def test_startup_time(self):
        """TC-P-004: 起動時間（初回）"""
        import shutil
        import os
        
        test_path = "./data/test_startup_db"
        if os.path.exists(test_path):
            shutil.rmtree(test_path)
        
        start = time.time()
        
        # 初期化
        client = OllamaClient()
        rag = RAGEngine(client, chroma_path=test_path)
        
        # 知識投入
        metadata_mapping = {
            "jetracer_tech.txt": {
                "domain": "technical",
                "character": "both"
            }
        }
        rag.init_from_files("./knowledge", metadata_mapping)
        
        elapsed = time.time() - start
        
        print(f"\n初回起動時間: {elapsed:.2f}秒")
        assert elapsed < 30.0  # 30秒以内
        
        # クリーンアップ
        shutil.rmtree(test_path)
```

### 6.6 pytest設定ファイル

```ini
# pytest.ini

[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

addopts = 
    -v
    --strict-markers
    --disable-warnings
    --cov=core
    --cov-report=term-missing
    --cov-report=html
    --tb=short

markers =
    slow: marks tests as slow (deselect with '-m "not slow"')
    integration: marks tests as integration tests
    performance: marks tests as performance tests
```

### 6.7 conftest.py（共通フィクスチャ）

```python
# tests/conftest.py

import pytest
import shutil
import os
from core.ollama_client import OllamaClient
from core.rag_engine import RAGEngine

@pytest.fixture(scope="session")
def ollama_client():
    """セッション全体で共有するOllamaClient"""
    return OllamaClient()

@pytest.fixture(scope="function")
def clean_test_db():
    """テスト用DBのクリーンアップ"""
    test_paths = [
        "./data/test_chroma_db",
        "./data/test_char_db",
        "./data/test_integration_db",
        "./data/test_perf_db",
        "./data/test_startup_db"
    ]
    
    # テスト前クリーンアップ
    for path in test_paths:
        if os.path.exists(path):
            shutil.rmtree(path)
    
    yield
    
    # テスト後クリーンアップ
    for path in test_paths:
        if os.path.exists(path):
            shutil.rmtree(path)
```

---

## 1. OllamaClient（Ollama接続層）

### 1.1 完全実装例

```python
# core/ollama_client.py

from openai import OpenAI
import ollama
import time
import logging
from typing import List, Dict, Optional

class OllamaClient:
    """
    Ollama接続クライアント
    
    easy-local-ragからの継承:
    - OpenAI互換APIの活用
    - ollama.embeddings()の直接利用
    
    duo-talk用の改良:
    - リトライ機構（exponential backoff）
    - タイムアウト処理
    - 詳細なエラーログ
    """
    
    def __init__(self, 
                 base_url: str = "http://localhost:11434/v1",
                 model: str = "gemma3:12b",
                 timeout: float = 30.0,
                 max_retries: int = 3):
        """
        Args:
            base_url: Ollama API URL
            model: 使用するLLMモデル名
            timeout: タイムアウト時間（秒）
            max_retries: リトライ最大回数
        """
        self.base_url = base_url
        self.model = model
        self.timeout = timeout
        self.max_retries = max_retries
        
        # OpenAI互換クライアント（LLM生成用）
        self.client = OpenAI(
            base_url=base_url,
            api_key="dummy"  # Ollamaはキー不要
        )
        
        self.logger = logging.getLogger(__name__)
    
    def generate(self, 
                messages: List[Dict[str, str]],
                temperature: float = 0.7,
                max_tokens: int = 2000) -> str:
        """
        テキスト生成（リトライ付き）
        
        Args:
            messages: OpenAI形式のメッセージ
                [
                    {"role": "system", "content": "..."},
                    {"role": "user", "content": "..."}
                ]
            temperature: 生成の多様性（0.0-2.0）
            max_tokens: 最大生成トークン数
        
        Returns:
            生成されたテキスト
        
        Raises:
            ConnectionError: 接続失敗
            TimeoutError: タイムアウト
        """
        for attempt in range(self.max_retries):
            try:
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    timeout=self.timeout
                )
                return response.choices[0].message.content
            
            except Exception as e:
                self.logger.warning(
                    f"生成失敗（試行 {attempt + 1}/{self.max_retries}）: {e}"
                )
                
                if attempt < self.max_retries - 1:
                    # Exponential backoff
                    wait_time = 2 ** attempt
                    self.logger.info(f"{wait_time}秒待機後にリトライ")
                    time.sleep(wait_time)
                else:
                    self.logger.error("最大リトライ回数を超過")
                    raise
    
    def embed(self, 
              text: str,
              model: str = "mxbai-embed-large") -> List[float]:
        """
        埋め込みベクトル生成
        
        Args:
            text: 埋め込み対象テキスト
            model: 埋め込みモデル名
        
        Returns:
            埋め込みベクトル（リスト形式）
        
        Note:
            easy-local-ragと同じollama.embeddings()を使用
        """
        try:
            response = ollama.embeddings(
                model=model,
                prompt=text
            )
            return response["embedding"]
        
        except Exception as e:
            self.logger.error(f"埋め込み生成失敗: {e}")
            raise
    
    def is_healthy(self) -> bool:
        """
        Ollama接続確認
        
        Returns:
            接続可能ならTrue
        """
        try:
            # シンプルなメッセージで確認
            self.generate(
                messages=[{"role": "user", "content": "test"}],
                max_tokens=10
            )
            return True
        except:
            return False
```

### 1.2 使用例

```python
# 初期化
client = OllamaClient(
    base_url="http://localhost:11434/v1",
    model="gemma3:12b"
)

# ヘルスチェック
if client.is_healthy():
    print("Ollama接続OK")

# テキスト生成
messages = [
    {"role": "system", "content": "あなたは親切なアシスタントです"},
    {"role": "user", "content": "こんにちは"}
]
response = client.generate(messages, temperature=0.7)
print(response)

# 埋め込み生成
embedding = client.embed("JetRacerとは自律走行車です")
print(f"ベクトル次元: {len(embedding)}")
```

---

## 2. RAGEngine（RAG検索エンジン）

### 2.1 完全実装例

```python
# core/rag_engine.py

import chromadb
from chromadb.config import Settings
from typing import List, Dict, Optional
import logging
import os

class RAGEngine:
    """
    RAG検索エンジン
    
    easy-local-ragからの継承:
    - コサイン類似度検索
    - チャンク分割（1000文字）
    
    duo-talk用の改良:
    - ChromaDB永続化
    - メタデータフィルタリング
    - 関連度スコア返却
    """
    
    def __init__(self, 
                 ollama_client,
                 chroma_path: str = "./data/chroma_db",
                 collection_name: str = "duo_knowledge"):
        """
        Args:
            ollama_client: OllamaClientインスタンス
            chroma_path: ChromaDB永続化パス
            collection_name: コレクション名
        """
        self.ollama = ollama_client
        self.chroma_path = chroma_path
        self.logger = logging.getLogger(__name__)
        
        # ChromaDB初期化
        os.makedirs(chroma_path, exist_ok=True)
        self.client = chromadb.PersistentClient(path=chroma_path)
        
        # コレクション取得or作成
        self.collection = self.client.get_or_create_collection(
            name=collection_name
        )
        
        self.logger.info(
            f"RAGEngine初期化完了: {self.collection.count()}件の知識"
        )
    
    def add_knowledge(self, 
                     texts: List[str],
                     metadatas: List[Dict[str, str]]):
        """
        知識追加
        
        Args:
            texts: テキストのリスト
            metadatas: メタデータのリスト
                例: {"domain": "technical", "character": "both"}
        """
        if len(texts) != len(metadatas):
            raise ValueError("texts と metadatas の長さが一致しません")
        
        # ID生成（タイムスタンプベース）
        import time
        base_id = int(time.time() * 1000)
        ids = [f"doc_{base_id}_{i}" for i in range(len(texts))]
        
        # 埋め込み生成
        embeddings = []
        for text in texts:
            emb = self.ollama.embed(text)
            embeddings.append(emb)
        
        # ChromaDBに追加
        self.collection.add(
            ids=ids,
            documents=texts,
            embeddings=embeddings,
            metadatas=metadatas
        )
        
        self.logger.info(f"{len(texts)}件の知識を追加")
    
    def search(self,
              query: str,
              top_k: int = 3,
              filters: Optional[Dict[str, str]] = None) -> List[Dict]:
        """
        類似度検索
        
        Args:
            query: 検索クエリ
            top_k: 取得件数
            filters: メタデータフィルタ
                例: {"character": "yana"}
        
        Returns:
            検索結果:
            [
                {
                    "text": "検索されたテキスト",
                    "score": 0.85,  # 類似度
                    "metadata": {"domain": "technical", ...}
                },
                ...
            ]
        """
        # クエリの埋め込み生成
        query_embedding = self.ollama.embed(query)
        
        # ChromaDBで検索
        results = self.collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k,
            where=filters  # メタデータフィルタ
        )
        
        # 結果整形
        formatted = []
        if results["documents"] and len(results["documents"][0]) > 0:
            for i in range(len(results["documents"][0])):
                formatted.append({
                    "text": results["documents"][0][i],
                    "score": 1 - results["distances"][0][i],  # 距離→類似度
                    "metadata": results["metadatas"][0][i]
                })
        
        self.logger.debug(f"検索: '{query}' → {len(formatted)}件")
        return formatted
    
    def init_from_files(self,
                       knowledge_dir: str,
                       metadata_mapping: Dict[str, Dict]):
        """
        ファイルから知識ベース初期化
        
        Args:
            knowledge_dir: 知識ファイルディレクトリ
            metadata_mapping: ファイル名→メタデータ
                例:
                {
                    "jetracer_tech.txt": {
                        "domain": "technical",
                        "character": "both"
                    }
                }
        """
        # 既に知識があればスキップ
        if self.collection.count() > 0:
            self.logger.info("既存知識を使用")
            return
        
        self.logger.info("知識ベース初期化開始")
        
        for filename, metadata in metadata_mapping.items():
            filepath = os.path.join(knowledge_dir, filename)
            
            if not os.path.exists(filepath):
                self.logger.warning(f"ファイル未発見: {filepath}")
                continue
            
            # ファイル読み込み
            with open(filepath, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # チャンク分割
            chunks = self._chunk_text(content, max_chars=1000)
            
            # メタデータにソース追加
            metadatas = []
            for _ in chunks:
                meta = metadata.copy()
                meta["source"] = filename
                metadatas.append(meta)
            
            # 知識追加
            self.add_knowledge(chunks, metadatas)
        
        self.logger.info(
            f"初期化完了: {self.collection.count()}件の知識"
        )
    
    def _chunk_text(self, text: str, max_chars: int = 1000) -> List[str]:
        """
        テキストをチャンク分割
        
        Args:
            text: 分割対象テキスト
            max_chars: 最大文字数
        
        Returns:
            チャンクのリスト
        
        Note:
            easy-local-ragと同じ方式（文単位で分割）
        """
        # 改行で分割
        lines = text.split('\n')
        
        chunks = []
        current_chunk = ""
        
        for line in lines:
            if len(current_chunk) + len(line) + 1 > max_chars:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = line
                else:
                    chunks.append(line[:max_chars])
            else:
                current_chunk += "\n" + line if current_chunk else line
        
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
```

### 2.2 使用例

```python
# 初期化
rag = RAGEngine(
    ollama_client=client,
    chroma_path="./data/chroma_db"
)

# ファイルから知識投入
rag.init_from_files(
    knowledge_dir="./knowledge",
    metadata_mapping={
        "jetracer_tech.txt": {
            "domain": "technical",
            "character": "both",
            "importance": "high"
        }
    }
)

# 検索
results = rag.search(
    query="JetRacerのセンサーについて",
    top_k=3,
    filters={"domain": "technical"}
)

for r in results:
    print(f"[{r['score']:.2f}] {r['text'][:50]}...")
```

---

## 3. Character（キャラクター応答）

### 3.1 完全実装例

```python
# core/character.py

import yaml
from typing import List, Dict, Optional
import logging

class Character:
    """
    キャラクター応答生成
    
    easy-local-ragからの継承:
    - 会話履歴管理
    - システムプロンプト活用
    
    duo-talk用の追加:
    - キャラクター設定（YAML）
    - Query Rewrite（オプション）
    - RAG検索統合
    """
    
    def __init__(self,
                 name: str,
                 config_path: str,
                 ollama_client,
                 rag_engine):
        """
        Args:
            name: キャラクター名（"yana" | "ayu"）
            config_path: 設定ファイルパス（YAML）
            ollama_client: OllamaClientインスタンス
            rag_engine: RAGEngineインスタンス
        """
        self.name = name
        self.ollama = ollama_client
        self.rag = rag_engine
        self.logger = logging.getLogger(f"{__name__}.{name}")
        
        # 設定読み込み
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
        
        # 会話履歴（最大10ターン）
        self.history: List[Dict[str, str]] = []
        self.max_history = 10
        
        self.logger.info(f"キャラクター初期化: {self.name}")
    
    def respond(self,
               user_input: str,
               use_rag: bool = True,
               rewrite_query: bool = False) -> str:
        """
        応答生成
        
        Args:
            user_input: ユーザー入力
            use_rag: RAG検索を使用するか
            rewrite_query: クエリ書き換えを使用するか
        
        Returns:
            応答テキスト
        """
        # 1. [オプション] Query Rewrite
        search_query = user_input
        if rewrite_query and len(self.history) > 0:
            search_query = self._rewrite_query(user_input)
            self.logger.debug(f"Query書き換え: {user_input} → {search_query}")
        
        # 2. RAG検索
        context = ""
        if use_rag:
            rag_results = self.rag.search(
                query=search_query,
                top_k=3,
                filters={"character": ["both", self.name]}  # 共通+自分
            )
            
            if rag_results:
                context_parts = [r["text"] for r in rag_results]
                context = "\n\n".join(context_parts)
        
        # 3. システムプロンプト構築
        system_prompt = self._build_system_prompt(context)
        
        # 4. メッセージ構築
        messages = [{"role": "system", "content": system_prompt}]
        
        # 会話履歴追加
        messages.extend(self.history)
        
        # ユーザー入力追加
        messages.append({"role": "user", "content": user_input})
        
        # 5. LLM生成
        response = self.ollama.generate(
            messages=messages,
            temperature=self.config.get("temperature", 0.7)
        )
        
        # 6. 履歴更新
        self._update_history(user_input, response)
        
        return response
    
    def _build_system_prompt(self, context: str) -> str:
        """
        システムプロンプト構築
        
        Args:
            context: RAG検索結果
        
        Returns:
            システムプロンプト
        """
        base_prompt = self.config["system_prompt"]
        
        if context:
            return f"""{base_prompt}

# 関連知識
以下の情報を参考にして応答してください：

{context}
"""
        return base_prompt
    
    def _rewrite_query(self, user_input: str) -> str:
        """
        クエリ書き換え（会話履歴を考慮）
        
        Args:
            user_input: 元のユーザー入力
        
        Returns:
            書き換え後のクエリ
        
        Note:
            easy-local-ragのQuery Rewrite機能を参考
        """
        # 直近2ターンを取得
        recent_history = self.history[-4:] if len(self.history) >= 4 else self.history
        
        context_str = ""
        for msg in recent_history:
            role = "ユーザー" if msg["role"] == "user" else "アシスタント"
            context_str += f"{role}: {msg['content']}\n"
        
        rewrite_prompt = f"""以下の会話履歴を踏まえて、ユーザーの最新の質問を具体的な検索クエリに書き換えてください。
代名詞や省略を解消し、検索に適した形にしてください。

会話履歴:
{context_str}

ユーザーの質問: {user_input}

書き換え後のクエリ（1文で簡潔に）:"""
        
        messages = [{"role": "user", "content": rewrite_prompt}]
        
        rewritten = self.ollama.generate(
            messages=messages,
            temperature=0.1  # 安定性重視
        )
        
        return rewritten.strip()
    
    def _update_history(self, user_input: str, response: str):
        """
        会話履歴更新
        
        Args:
            user_input: ユーザー入力
            response: AI応答
        """
        self.history.append({"role": "user", "content": user_input})
        self.history.append({"role": "assistant", "content": response})
        
        # 最大履歴数を超えたら古いものを削除
        while len(self.history) > self.max_history * 2:  # user+assistant=2
            self.history.pop(0)
    
    def clear_history(self):
        """会話履歴をクリア"""
        self.history = []
        self.logger.info("会話履歴をクリア")
```

### 3.2 使用例

```python
# キャラクター初期化
yana = Character(
    name="yana",
    config_path="./personas/yana.yaml",
    ollama_client=client,
    rag_engine=rag
)

# 応答生成
response = yana.respond(
    user_input="JetRacerのセンサーって何があるの？",
    use_rag=True,
    rewrite_query=False
)
print(f"やな: {response}")

# 会話継続
response = yana.respond(
    user_input="それって正確なの？",
    use_rag=True,
    rewrite_query=True  # 「それ」を解消
)
print(f"やな: {response}")

# 会話履歴クリア
yana.clear_history()
```

---

## 4. Application（メインスクリプト）

### 4.1 完全実装例

```python
# chat.py

import yaml
import logging
from core.ollama_client import OllamaClient
from core.rag_engine import RAGEngine
from core.character import Character

def main():
    # ログ設定
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # 設定読み込み
    with open('config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # コンポーネント初期化
    print("システム初期化中...")
    
    # 1. OllamaClient
    ollama_config = config['ollama']
    client = OllamaClient(
        base_url=ollama_config['base_url'],
        model=ollama_config['model'],
        timeout=ollama_config.get('timeout', 30.0),
        max_retries=ollama_config.get('max_retries', 3)
    )
    
    if not client.is_healthy():
        print("エラー: Ollamaに接続できません")
        return
    
    # 2. RAGEngine
    rag_config = config['rag']
    rag = RAGEngine(
        ollama_client=client,
        chroma_path=rag_config['chroma_path']
    )
    
    # 知識ベース初期化
    if rag.collection.count() == 0:
        print("知識ベース初期化中...")
        metadata_mapping = {}
        for source in config['knowledge_sources']:
            metadata_mapping[source['file']] = source['metadata']
        
        rag.init_from_files(
            knowledge_dir="./knowledge",
            metadata_mapping=metadata_mapping
        )
    
    # 3. Characters
    characters = {}
    for char_name, char_config in config['characters'].items():
        characters[char_name] = Character(
            name=char_name,
            config_path=char_config['config_path'],
            ollama_client=client,
            rag_engine=rag
        )
    
    print("初期化完了！\n")
    
    # 会話ループ
    current_char = None
    
    print("=== duo-talk チャットシステム ===")
    print("コマンド: /exit（終了）, /clear（履歴クリア）, /switch（キャラ変更）\n")
    
    while True:
        # キャラクター選択
        if current_char is None:
            print(f"キャラクター選択: {', '.join(characters.keys())}")
            char_input = input("キャラクター> ").strip()
            
            if char_input not in characters:
                print(f"エラー: '{char_input}' は存在しません")
                continue
            
            current_char = characters[char_input]
            print(f"\n{current_char.name} と会話を開始します\n")
        
        # ユーザー入力
        user_input = input(f"{current_char.name}> ").strip()
        
        if not user_input:
            continue
        
        # コマンド処理
        if user_input == "/exit":
            print("終了します")
            break
        
        elif user_input == "/clear":
            current_char.clear_history()
            print("会話履歴をクリアしました\n")
            continue
        
        elif user_input == "/switch":
            current_char = None
            print()
            continue
        
        # 応答生成
        try:
            response = current_char.respond(user_input)
            print(f"\n{current_char.name}: {response}\n")
        
        except Exception as e:
            print(f"エラー: {e}\n")

if __name__ == "__main__":
    main()
```

### 4.2 使用例

```bash
# システム起動
python chat.py

# 出力例:
# システム初期化中...
# 初期化完了！
#
# === duo-talk チャットシステム ===
# コマンド: /exit（終了）, /clear（履歴クリア）, /switch（キャラ変更）
#
# キャラクター選択: yana, ayu
# キャラクター> yana
#
# yana と会話を開始します
#
# yana> こんにちは
#
# yana: こんにちは！JetRacerの調子はどう？
```

---

## 5. コンポーネント連携パターン

### 5.1 初期化シーケンス

```python
# 依存関係の順序通りに初期化
client = OllamaClient(...)           # 1. 最下層
rag = RAGEngine(client, ...)         # 2. RAG層
character = Character(..., client, rag)  # 3. Character層
```

### 5.2 応答生成フロー

```python
# ユーザー入力から応答までの完全フロー
user_input = "JetRacerのセンサーは？"

# Character内部で以下が実行される:
# 1. クエリ書き換え（オプション）
query = character._rewrite_query(user_input)

# 2. RAG検索
results = rag.search(query, top_k=3)

# 3. プロンプト構築
system_prompt = character._build_system_prompt(results)

# 4. LLM生成
response = client.generate(messages=[
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_input}
])

# 5. 履歴更新
character._update_history(user_input, response)
```

---

## 6. テストコード例

### 6.1 OllamaClient テスト

```python
# tests/test_ollama_client.py

import pytest
from core.ollama_client import OllamaClient

def test_ollama_health():
    """Ollama接続テスト"""
    client = OllamaClient()
    assert client.is_healthy() == True

def test_generate():
    """テキスト生成テスト"""
    client = OllamaClient()
    messages = [{"role": "user", "content": "こんにちは"}]
    response = client.generate(messages)
    assert len(response) > 0

def test_embed():
    """埋め込み生成テスト"""
    client = OllamaClient()
    embedding = client.embed("テスト")
    assert len(embedding) == 1024  # mxbai-embed-largeは1024次元
```

### 6.2 RAGEngine テスト

```python
# tests/test_rag_engine.py

import pytest
from core.ollama_client import OllamaClient
from core.rag_engine import RAGEngine
import tempfile
import shutil

@pytest.fixture
def rag_engine():
    """RAGEngineテスト用フィクスチャ"""
    client = OllamaClient()
    temp_dir = tempfile.mkdtemp()
    rag = RAGEngine(client, chroma_path=temp_dir)
    
    yield rag
    
    # クリーンアップ
    shutil.rmtree(temp_dir)

def test_add_and_search(rag_engine):
    """知識追加と検索テスト"""
    # 知識追加
    rag_engine.add_knowledge(
        texts=["JetRacerは自律走行車です"],
        metadatas=[{"domain": "technical"}]
    )
    
    # 検索
    results = rag_engine.search("自律走行", top_k=1)
    assert len(results) > 0
    assert "JetRacer" in results[0]["text"]
```

---

**作成日**: 2026年1月14日  
**最終更新**: 2026年1月14日  
**参照**: 01_アーキテクチャ設計.md, 02_コンポーネント詳細.md
