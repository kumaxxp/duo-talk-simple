# コンポーネント詳細設計

**目的**: 各コンポーネントの責務、API仕様、動作フローを詳細に定義

**実装サンプル**: 実際のコード例は [07_実装サンプル.md](./07_実装サンプル.md) を参照

---

## 0. Application Layer（アプリケーション層）

### 0.1 概要

**ファイル**: `chat.py`

**責務**:
- システム全体の初期化と起動
- ユーザーインタラクションの管理
- キャラクター選択インターフェース
- 会話ループの制御
- コマンド処理

**依存関係**:
- `core.ollama_client.OllamaClient`
- `core.rag_engine.RAGEngine`
- `core.character.Character`
- `pyyaml`: 設定ファイル読み込み

### 0.2 主要機能

#### 初期化シーケンス

1. `config.yaml` 読み込み
2. `OllamaClient` 生成
3. Ollama接続確認（`is_healthy()`）
4. `RAGEngine` 生成
5. 知識ベース初期化（初回のみ）
6. `Character` インスタンス生成（yana, ayu）

#### 会話ループ

```
while True:
    if current_char is None:
        キャラクター選択
    
    user_input = input()
    
    if コマンド:
        コマンド処理
    else:
        response = current_char.respond(user_input)
        print(response)
```

#### コマンド処理

| コマンド | 機能 |
|---------|------|
| `/exit` | システム終了 |
| `/clear` | 会話履歴クリア |
| `/switch` | キャラクター切り替え |

### 0.3 エラー処理

- Ollama接続失敗 → エラーメッセージ表示、終了
- 設定ファイル不正 → エラーメッセージ、デフォルト値使用
- 応答生成エラー → エラーメッセージ表示、会話継続

---

## 1. Character（キャラクター応答層）

### 1.1 概要

**ファイル**: `core/character.py`

**責務**:
- キャラクター設定（YAML）の読み込みと管理
- 会話履歴の管理
- RAG検索結果の統合
- システムプロンプトの構築
- LLM応答の生成
- クエリ書き換え（オプション）

**easy-local-ragからの継承**:
- 会話履歴管理の仕組み
- システムプロンプト活用

**duo-talk用の追加**:
- YAML設定によるキャラクター定義
- RAG検索統合
- Query Rewrite機能

### 1.2 API仕様

#### コンストラクタ

```python
def __init__(self,
             name: str,
             config_path: str,
             ollama_client: OllamaClient,
             rag_engine: RAGEngine)
```

**引数**:
- `name`: キャラクター名（"yana" | "ayu"）
- `config_path`: YAML設定ファイルパス
- `ollama_client`: OllamaClientインスタンス
- `rag_engine`: RAGEngineインスタンス

**初期化処理**:
1. YAML設定読み込み
2. 会話履歴初期化（空リスト）
3. ロガー設定

#### respond() - メイン応答生成

```python
def respond(self,
           user_input: str,
           use_rag: bool = True,
           rewrite_query: bool = False) -> str
```

**引数**:
- `user_input`: ユーザー入力テキスト
- `use_rag`: RAG検索を使用するか（デフォルト: True）
- `rewrite_query`: クエリ書き換えを使用するか（デフォルト: False）

**返り値**:
- 生成された応答テキスト

**処理フロー**:
1. [オプション] クエリ書き換え
2. RAG検索
3. システムプロンプト構築
4. メッセージ配列構築
5. LLM生成
6. 会話履歴更新

#### clear_history() - 履歴クリア

```python
def clear_history(self) -> None
```

会話履歴を空にする。

### 1.3 内部メソッド

#### _build_system_prompt() - プロンプト構築

```python
def _build_system_prompt(self, context: str) -> str
```

**処理**:
- base_prompt（YAML設定から）を取得
- contextがある場合、関連知識セクションを追加

**出力形式**:
```
{base_prompt}

# 関連知識
以下の情報を参考にして応答してください：

{context}
```

#### _rewrite_query() - クエリ書き換え

```python
def _rewrite_query(self, user_input: str) -> str
```

**処理**:
1. 直近2ターン（4メッセージ）を取得
2. 会話履歴を文字列化
3. LLMに書き換えを依頼（temperature=0.1）
4. 書き換え後のクエリを返却

**用途**:
- 「それ」「あれ」などの代名詞解消
- 省略された情報の補完

#### _update_history() - 履歴更新

```python
def _update_history(self, user_input: str, response: str) -> None
```

**処理**:
1. user入力を追加
2. assistant応答を追加
3. 最大履歴数（10ターン＝20メッセージ）を超えたら古いものを削除

### 1.4 データ構造

#### 会話履歴

```python
history: List[Dict[str, str]] = [
    {"role": "user", "content": "ユーザー入力1"},
    {"role": "assistant", "content": "AI応答1"},
    {"role": "user", "content": "ユーザー入力2"},
    {"role": "assistant", "content": "AI応答2"},
    ...
]
```

**制限**: 最大10ターン（20メッセージ）

#### キャラクター設定（YAML）

```yaml
name: "yana"
system_prompt: |
  あなたは「やな」という名前の姉です。
  直感的で行動派。「〜じゃん」と話します。

generation:
  temperature: 0.8
  max_tokens: 2000
```

---

## 2. RAGEngine（検索拡張層）

### 2.1 概要

**ファイル**: `core/rag_engine.py`

**責務**:
- 知識のチャンク分割と保存
- ChromaDBへの永続化
- 類似度検索
- メタデータフィルタリング
- 関連度スコア計算

**easy-local-ragからの継承**:
- コサイン類似度検索
- チャンク分割（1000文字単位）

**duo-talk用の追加**:
- ChromaDB永続化
- メタデータフィルタリング
- スコア返却

### 2.2 API仕様

#### コンストラクタ

```python
def __init__(self,
             ollama_client: OllamaClient,
             chroma_path: str = "./data/chroma_db",
             collection_name: str = "duo_knowledge")
```

**引数**:
- `ollama_client`: OllamaClientインスタンス
- `chroma_path`: ChromaDB永続化ディレクトリ
- `collection_name`: コレクション名

**初期化処理**:
1. ChromaDB永続化ディレクトリ作成
2. ChromaDBクライアント初期化
3. コレクション取得or作成

#### add_knowledge() - 知識追加

```python
def add_knowledge(self,
                 texts: List[str],
                 metadatas: List[Dict[str, str]]) -> None
```

**引数**:
- `texts`: テキストチャンクのリスト
- `metadatas`: メタデータのリスト（textsと同じ長さ）

**処理フロー**:
1. バリデーション（texts と metadatas の長さ一致）
2. ID生成（タイムスタンプベース）
3. 各テキストの埋め込み生成（`ollama_client.embed()`）
4. ChromaDBに追加

#### search() - 類似度検索

```python
def search(self,
          query: str,
          top_k: int = 3,
          filters: Optional[Dict[str, str]] = None) -> List[Dict]
```

**引数**:
- `query`: 検索クエリ
- `top_k`: 取得件数
- `filters`: メタデータフィルタ（例: `{"character": "yana"}`）

**返り値**:
```python
[
    {
        "text": "検索されたテキスト",
        "score": 0.85,  # 類似度（0.0-1.0）
        "metadata": {"domain": "technical", ...}
    },
    ...
]
```

**処理フロー**:
1. クエリの埋め込み生成
2. ChromaDBで検索（`where`パラメータでフィルタ）
3. 距離を類似度に変換（`score = 1 - distance`）
4. 結果整形

#### init_from_files() - ファイルから初期化

```python
def init_from_files(self,
                   knowledge_dir: str,
                   metadata_mapping: Dict[str, Dict]) -> None
```

**引数**:
- `knowledge_dir`: 知識ファイルディレクトリ
- `metadata_mapping`: ファイル名→メタデータのマッピング

**処理フロー**:
1. コレクション件数チェック（既存データがあればスキップ）
2. 各ファイルを読み込み
3. チャンク分割（`_chunk_text()`）
4. メタデータにソース情報追加
5. `add_knowledge()` 呼び出し

### 2.3 内部メソッド

#### _chunk_text() - テキスト分割

```python
def _chunk_text(self, text: str, max_chars: int = 1000) -> List[str]
```

**アルゴリズム**:
1. 改行で行に分割
2. max_chars以内で蓄積
3. 超えたら新しいチャンク開始

**特徴**:
- 文章の途中で切らない
- easy-local-ragと同じ方式

### 2.4 データ構造

#### メタデータ

```python
{
    "domain": "technical" | "character" | "experience",
    "character": "both" | "yana" | "ayu",
    "importance": "high" | "medium" | "low",
    "source": "ファイル名"
}
```

#### ChromaDBコレクション

```python
{
    "id": "doc_1234567890_0",
    "document": "テキストチャンク内容",
    "embedding": [0.123, -0.456, ...],  # 1024次元
    "metadata": {...}
}
```

---

## 3. OllamaClient（言語モデル接続層）

### 3.1 概要

**ファイル**: `core/ollama_client.py`

**責務**:
- Ollama APIとの通信
- テキスト生成
- 埋め込みベクトル生成
- 接続確認
- エラー処理とリトライ

**easy-local-ragからの継承**:
- OpenAI互換API活用
- `ollama.embeddings()` 直接利用

**duo-talk用の追加**:
- リトライ機構（exponential backoff）
- タイムアウト処理
- 詳細なエラーログ

### 3.2 API仕様

#### コンストラクタ

```python
def __init__(self,
             base_url: str = "http://localhost:11434/v1",
             model: str = "gemma3:12b",
             timeout: float = 30.0,
             max_retries: int = 3)
```

**引数**:
- `base_url`: Ollama API URL
- `model`: 使用するLLMモデル名
- `timeout`: タイムアウト時間（秒）
- `max_retries`: リトライ最大回数

**初期化処理**:
1. OpenAI互換クライアント生成
2. ロガー設定

#### generate() - テキスト生成

```python
def generate(self,
            messages: List[Dict[str, str]],
            temperature: float = 0.7,
            max_tokens: int = 2000) -> str
```

**引数**:
- `messages`: OpenAI形式のメッセージ配列
- `temperature`: 生成の多様性（0.0-2.0）
- `max_tokens`: 最大生成トークン数

**返り値**:
- 生成されたテキスト

**エラー処理**:
1. 例外キャッチ
2. ログ出力
3. exponential backoff（2^attempt 秒待機）
4. 最大リトライ回数超過で例外再送出

#### embed() - 埋め込み生成

```python
def embed(self,
         text: str,
         model: str = "mxbai-embed-large") -> List[float]
```

**引数**:
- `text`: 埋め込み対象テキスト
- `model`: 埋め込みモデル名

**返り値**:
- 埋め込みベクトル（1024次元のリスト）

**実装**:
- `ollama.embeddings()` を直接利用（easy-local-ragと同じ）

#### is_healthy() - 接続確認

```python
def is_healthy(self) -> bool
```

**処理**:
1. シンプルなテキスト生成を試行
2. 成功 → True
3. 失敗 → False

### 3.3 エラー処理戦略

| エラー種別 | 検出 | 対応 |
|-----------|------|------|
| ConnectionError | Ollamaが起動していない | リトライ3回 → 例外送出 |
| TimeoutError | 応答が30秒超過 | 例外送出 |
| ModelNotFoundError | モデル未ダウンロード | エラーログ → 例外送出 |

### 3.4 リトライアルゴリズム

**Exponential Backoff**:
```
attempt 0: 即座にリトライ
attempt 1: 2^1 = 2秒待機
attempt 2: 2^2 = 4秒待機
attempt 3以降: 例外送出
```

---

## 4. Infrastructure Layer（インフラ層）

### 4.1 ChromaDB

**ディレクトリ**: `data/chroma_db/`

**自動生成**: 初回起動時にRAGEngineが作成

**データ形式**:
- SQLite + バイナリファイル
- ディスク使用量: 知識量に依存（100-500MB程度）

**検索性能**:
- 類似度検索: < 0.5秒
- top-k=3で十分高速

### 4.2 YAML設定ファイル

#### config.yaml - システム設定

**場所**: プロジェクトルート

**構造**:
```yaml
ollama:
  base_url: string
  model: string
  embed_model: string
  timeout: float
  max_retries: int

rag:
  chroma_path: string
  top_k: int
  chunk_size: int

characters:
  yana:
    config_path: string
  ayu:
    config_path: string

knowledge_sources:
  - file: string
    metadata:
      domain: string
      character: string
      importance: string
```

#### personas/*.yaml - キャラクター設定

**場所**: `personas/`ディレクトリ

**構造**:
```yaml
name: string
system_prompt: string (複数行可)

generation:
  temperature: float
  max_tokens: int
```

### 4.3 知識ベーステキストファイル

**場所**: `knowledge/`ディレクトリ

**フォーマット**:
- UTF-8エンコードのプレーンテキスト
- Markdown風の構造化推奨

**メタデータマッピング**:
- `config.yaml` の `knowledge_sources` で定義

---

## 5. コンポーネント連携

### 5.1 依存関係グラフ

```
Application (chat.py)
    ↓
Character
    ↓ ↓
    │ RAGEngine
    │     ↓
    └→ OllamaClient
         ↓
    Infrastructure
    (ChromaDB, YAML, knowledge files)
```

### 5.2 初期化シーケンス

1. Application が config.yaml 読み込み
2. OllamaClient 生成
3. RAGEngine 生成（OllamaClient渡す）
4. RAGEngine.init_from_files()（初回のみ）
5. Character生成（OllamaClient, RAGEngine渡す）

### 5.3 応答生成シーケンス

```
ユーザー入力
    ↓
Application → Character.respond()
    ↓
Character → [オプション] _rewrite_query()
    ↓           ↓
    │       OllamaClient.generate()
    ↓
Character → RAGEngine.search()
    ↓           ↓
    │       OllamaClient.embed()
    ↓           ↓
    │       ChromaDB検索
    ↓
Character → _build_system_prompt()
    ↓
Character → OllamaClient.generate()
    ↓
Character → _update_history()
    ↓
応答返却
```

---

## 6. パフォーマンス最適化

### 6.1 埋め込みキャッシュ

- ChromaDBが埋め込みを保存
- 2回目以降は再計算不要

### 6.2 会話履歴制限

- 最大10ターン（20メッセージ）
- メモリ使用量を抑制
- コンテキスト長超過を防止

### 6.3 バッチ処理

- 知識投入時、複数チャンクをまとめて処理
- 埋め込み生成のオーバーヘッド削減

---

## 7. 拡張性への配慮

### 7.1 設定の外部化

すべての設定をYAMLファイルに定義することで：
- コード修正不要で動作変更可能
- キャラクター追加が容易
- 知識ベース追加が容易

### 7.2 インターフェース統一

各コンポーネントが明確なAPIを持つことで：
- テストが容易
- 実装の差し替えが可能
- 並行開発が可能

### 7.3 依存関係の明確化

上位層は下位層のみに依存するため：
- 下位層の変更が上位層に影響しにくい
- モックによるテストが容易

---

**作成日**: 2026年1月14日  
**最終更新**: 2026年1月14日（コンポーネント順序統一、実装コード分離）  
**実装サンプル**: [07_実装サンプル.md](./07_実装サンプル.md)
