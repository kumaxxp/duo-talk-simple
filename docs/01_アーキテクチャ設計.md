# アーキテクチャ設計

## システム全体構成

### レイヤーアーキテクチャ

```
┌─────────────────────────────────────────────────────────┐
│  Application Layer (アプリケーション層)                  │
│  chat.py                                                 │
│                                                          │
│  - ユーザーインタラクション                              │
│  - キャラクター選択                                      │
│  - 会話ループ管理                                        │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│  Character Layer (キャラクター応答層)                    │
│  Character クラス (core/character.py)                    │
│                                                          │
│  - キャラクター性の表現                                  │
│  - プロンプト構築                                        │
│  - 会話履歴管理                                          │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│  RAGEngine Layer (検索拡張層)                            │
│  RAGEngine クラス (core/rag_engine.py)                   │
│                                                          │
│  - 知識検索                                              │
│  - コンテキスト生成                                      │
│  - メタデータフィルタリング                              │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│  OllamaClient Layer (言語モデル接続層)                   │
│  OllamaClient クラス (core/ollama_client.py)             │
│                                                          │
│  - Ollama接続管理                                        │
│  - テキスト生成                                          │
│  - 埋め込みベクトル生成                                  │
└─────────────────────────────────────────────────────────┘
                          ↓
┌─────────────────────────────────────────────────────────┐
│  Infrastructure Layer (インフラ層)                       │
│                                                          │
│  - ChromaDB (ベクトルストレージ)                         │
│  - YAML設定ファイル                                      │
│  - 知識ベーステキストファイル                            │
└─────────────────────────────────────────────────────────┘
```

### 依存関係ルール

```
上位層は下位層に依存できる
下位層は上位層に依存してはいけない

OK:  Character → RAGEngine → OllamaClient
NG:  OllamaClient → RAGEngine
```

---

## コンポーネント構成

### 0. Application Layer（アプリケーション層）

**責務**: ユーザーとシステムの橋渡し

```python
# chat.py

┌──────────────────────────────┐
│  chat.py                     │
│  (メインスクリプト)           │
│                              │
│  - 設定ファイル読み込み       │
│  - コンポーネント初期化       │
│  - キャラクター選択           │
│  - 会話ループ                 │
│  - コマンド処理               │
└──────────────────────────────┘
            ↓
    ┌─────────────────┐
    │  Character      │
    │  (yana / ayu)   │
    └─────────────────┘
```

**主要機能**:
- config.yamlの読み込みと検証
- OllamaClient, RAGEngine, Characterの初期化
- キャラクター選択インターフェース
- 会話ループ管理（入力→応答→表示）
- コマンド処理（/clear, /switch, /exit）
- エラーハンドリングと表示

**会話ループの流れ**:
```python
while True:
    # キャラクター選択
    if current_char is None:
        char_input = input("キャラクター> ")
        current_char = characters[char_input]
    
    # ユーザー入力
    user_input = input(f"{current_char.name}> ")
    
    # コマンド処理
    if user_input == "/exit":
        break
    elif user_input == "/clear":
        current_char.clear_history()
        continue
    
    # 応答生成
    response = current_char.respond(user_input)
    print(f"{current_char.name}: {response}")
```

**依存関係**:
- `pyyaml`: 設定ファイル読み込み
- `logging`: ログ出力
- `core.ollama_client`: OllamaClient
- `core.rag_engine`: RAGEngine
- `core.character`: Character

---

### 1. Character（キャラクター応答層）

**責務**: キャラクター性を持った応答生成

```python
┌──────────────────────────────┐
│  Character                   │
│                              │
│  + respond()                 │  # メイン応答生成
│  + clear_history()           │  # 履歴クリア
│  - _build_system_prompt()   │  # プロンプト構築
│  - _rewrite_query()          │  # クエリ書き換え
└──────────────────────────────┘
            ↓
    ┌─────────────────┐
    │  RAGEngine      │
    └─────────────────┘
            ↓
    ┌─────────────────┐
    │  OllamaClient   │
    └─────────────────┘
```

**主要機能**:
- キャラクター設定（YAML）の読み込み
- システムプロンプト構築
- 会話履歴管理（最大10ターン）
- クエリ書き換え（オプション）
- RAG検索結果の統合
- 応答生成

**会話履歴構造**:
```python
history = [
    {"role": "user", "content": "ユーザー入力"},
    {"role": "assistant", "content": "AI応答"},
    ...
]
```

**依存関係**:
- `RAGEngine`: コンテキスト検索用
- `OllamaClient`: 応答生成用
- `pyyaml`: 設定ファイル読み込み

---

### 2. RAGEngine（検索拡張層）

**責務**: 知識検索とコンテキスト生成

```python
┌──────────────────────────────┐
│  RAGEngine                   │
│                              │
│  + add_knowledge()           │  # 知識追加
│  + search()                  │  # 類似度検索
│  + init_from_files()         │  # ファイルから初期化
│  - _chunk_text()             │  # テキスト分割
└──────────────────────────────┘
            ↓ (依存)
     [Infrastructure Layer]
┌──────────────────────────────┐
│  ChromaDB                    │
│  (ベクトルストレージ)         │
│                              │
│  Collection: "duo_knowledge" │
│  - documents: テキストチャンク│
│  - embeddings: ベクトル       │
│  - metadata: 属性情報         │
└──────────────────────────────┘
```

**主要機能**:
- テキストの自動チャンク分割（1000文字単位）
- ChromaDBへの永続化
- メタデータ付き検索（キャラクター・ドメイン別）
- top-k類似度検索
- 関連度スコアの返却

**メタデータ構造**:
```python
{
    "domain": "technical" | "character" | "experience",
    "character": "both" | "yana" | "ayu",
    "importance": "high" | "medium" | "low",
    "source": "ファイル名"
}
```

**依存関係**:
- `chromadb`: ベクトルデータベース
- `OllamaClient`: 埋め込み生成用

---

### 3. OllamaClient（言語モデル接続層）

**責務**: Ollamaとの通信を一手に担う

```python
┌──────────────────────────────┐
│  OllamaClient                │
│                              │
│  + generate()                │  # テキスト生成
│  + embed()                   │  # 埋め込み生成
│  + is_healthy()              │  # ヘルスチェック
│  - _retry()                  │  # リトライ処理
└──────────────────────────────┘
            ↓
    Ollama API (HTTP)
    http://localhost:11434/v1
```

**主要機能**:
- OpenAI互換API経由でのテキスト生成
- mxbai-embed-largeによる埋め込み生成
- 自動リトライ（exponential backoff）
- タイムアウト処理
- エラーログ記録

**依存関係**:
- `openai`: OpenAI互換APIクライアント
- `ollama`: Ollama Python SDK（埋め込み用）

---

### 4. Infrastructure Layer（インフラ層）

**責務**: データ永続化と設定管理

```
┌─────────────────────────────────────────────────────────┐
│  Infrastructure Layer (インフラ層)                       │
│                                                          │
│  ┌────────────────────────────────────────────────────┐  │
│  │  ChromaDB                                         │  │
│  │  (ベクトルデータベース)                             │  │
│  │  - data/chroma_db/                                │  │
│  │  - 永続化ストレージ                                  │  │
│  │  - コレクション管理                                  │  │
│  └────────────────────────────────────────────────────┘  │
│                                                          │
│  ┌────────────────────────────────────────────────────┐  │
│  │  YAML設定ファイル                                 │  │
│  │  - config.yaml (システム設定)                       │  │
│  │  - personas/*.yaml (キャラ設定)                    │  │
│  │  - 宣言的設定管理                                  │  │
│  └────────────────────────────────────────────────────┘  │
│                                                          │
│  ┌────────────────────────────────────────────────────┐  │
│  │  知識ベーステキストファイル                        │  │
│  │  - knowledge/*.txt                                │  │
│  │  - ドメイン知識のソース                              │  │
│  │  - プレーンテキスト形式                              │  │
│  └────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────┘
```

#### 4.1 ChromaDB（ベクトルデータベース）

**責務**: 知識ベースの永続化と検索

**主要機能**:
- ベクトルデータのローカル永続化
- コサイン類似度検索
- メタデータフィルタリング
- コレクション管理

**データ構造**:
```python
# コレクション: "duo_knowledge"
{
    "id": "chunk_001",
    "document": "テキストチャンク内容",
    "embedding": [0.123, -0.456, ...],  # 1024次元ベクトル
    "metadata": {
        "domain": "technical",
        "character": "both",
        "importance": "high",
        "source": "jetracer_tech.txt"
    }
}
```

**ファイル配置**:
- `data/chroma_db/` → 自動生成される永続化ディレクトリ
- SQLite + バイナリファイルで管理
- 初回起動時に自動作成

**特徴**:
- ローカル実行（ネットワーク不要）
- 軽量（ディスク使用量 ~100-500MB）
- 高速検索（<0.5秒）

---

#### 4.2 YAML設定ファイル

**責務**: システム設定の宣言的管理

**ファイル構成**:

1. **config.yaml** - システム全体設定
```yaml
ollama:
  base_url: "http://localhost:11434/v1"
  model: "gemma3:12b"
  embed_model: "mxbai-embed-large"
  timeout: 30.0
  max_retries: 3

rag:
  chroma_path: "./data/chroma_db"
  top_k: 3
  chunk_size: 1000

characters:
  yana:
    config_path: "./personas/yana.yaml"
  ayu:
    config_path: "./personas/ayu.yaml"

knowledge_sources:
  - file: "knowledge/jetracer_tech.txt"
    metadata:
      domain: "technical"
      character: "both"
      importance: "high"
```

2. **personas/*.yaml** - キャラクター設定
```yaml
name: "yana"
system_prompt: |
  あなたは「やな」という名前の姉です。
  直感的で行動派。「〜じゃん」と話します。

generation:
  temperature: 0.8
  max_tokens: 2000
```

**特徴**:
- 人間が読み書きしやすい
- バージョン管理が容易
- プログラム修正不要で設定変更可能

---

#### 4.3 知識ベーステキストファイル

**責動**: ドメイン知識のソース

**ファイル例**:

1. **knowledge/jetracer_tech.txt** - JetRacer技術知識
```
JetRacerはNVIDIA Jetson Nanoを搭載した自律走行ロボットです。

センサー構成:
- 超音波距離センサー (HC-SR04) x 3
- CSIカメラモジュール (IMX219)
...
```

2. **knowledge/sisters_shared.txt** - 姉妹の共有知識
```
やなとあゆはJetRacerを共同で運用しています。

やなは直感的な制御を担当。
あゆはデータ分析と最適化を担当。
...
```

**フォーマット**:
- プレーンテキスト (UTF-8)
- Markdown風の構造化可
- 1ファイル = 1ドメイン/トピック

**特徴**:
- 編集が容易（テキストエディタでOK）
- バージョン管理がGitで可能
- 追加・変更がシステム再起動不要（再初期化のみ）

---

**Infrastructure Layerのポイント**:
- アプリケーションロジックから分離
- データ・設定の一元管理
- 人間が直接編集可能
- 再現性・ポータビリティが高い

---

## データフロー

### 基本会話フロー

```
1. ユーザー入力
   ↓
2. chat.py
   - キャラクター選択（yana/ayu）
   - Character.respond()呼び出し
   ↓
3. Character.respond()
   ├─ [Optional] Query Rewrite
   │  └─ 会話履歴 → LLM → 書き換え後の質問
   ├─ RAGEngine.search()
   │  └─ 質問の埋め込み → ChromaDB検索 → 関連知識
   ├─ システムプロンプト構築
   │  └─ base_prompt + context + history
   └─ OllamaClient.generate()
      └─ LLM生成 → 応答テキスト
   ↓
4. 応答表示
   ↓
5. 会話履歴に追加
```

### 初回起動時のフロー

```
1. RAGEngine初期化
   ↓
2. ChromaDBチェック
   - 既存データあり → ロード
   - 既存データなし → 以下を実行
   ↓
3. knowledge/内のテキストファイル読み込み
   ↓
4. テキスト分割（1000文字チャンク）
   ↓
5. 各チャンクの埋め込み生成
   - OllamaClient.embed() → mxbai-embed-large
   ↓
6. ChromaDBに保存
   - documents: テキスト
   - embeddings: ベクトル
   - metadata: メタデータ
   ↓
7. 完了（次回以降は高速起動）
```

---

## ファイル構成

### ディレクトリツリー

```
duo-talk-simple/
│
├── README.md                    # プロジェクト概要
├── requirements.txt             # 依存関係
├── config.yaml                  # 全体設定
├── chat.py                      # メインスクリプト
│
├── docs/                        # 技術資料
│   ├── 00_プロジェクト概要.md
│   ├── 01_アーキテクチャ設計.md  (本ファイル)
│   ├── 02_コンポーネント詳細.md
│   ├── 03_設定ファイル仕様.md
│   ├── 04_データフロー.md
│   ├── 05_実装計画.md
│   ├── 06_テスト戦略.md
│   └── 99_easy-local-rag比較分析.md
│
├── core/                        # コア機能（3ファイル）
│   ├── __init__.py
│   ├── ollama_client.py        # Ollama接続層
│   ├── rag_engine.py           # RAG検索エンジン
│   └── character.py            # キャラクター応答
│
├── knowledge/                   # 知識ベース
│   ├── jetracer_tech.txt       # JetRacer技術知識
│   ├── autonomy_basics.txt     # 自律走行基礎
│   └── sisters_shared.txt      # 姉妹の共有知識
│
├── personas/                    # キャラクター設定
│   ├── yana.yaml               # やな（姉）
│   └── ayu.yaml                # あゆ（妹）
│
├── data/                        # 実行時生成データ
│   └── chroma_db/              # ChromaDB永続化
│       └── (自動生成)
│
└── tests/                       # テスト
    ├── __init__.py
    ├── test_ollama_client.py
    ├── test_rag_engine.py
    ├── test_character.py
    └── test_integration.py
```

---

## インタフェース設計

**注意**: 実装サンプルコードは [07_実装サンプル.md](./07_実装サンプル.md) を参照してください。

### OllamaClient API

```python
class OllamaClient:
    def __init__(self, base_url: str, model: str, 
                 timeout: float = 30.0, max_retries: int = 3):
        """
        Args:
            base_url: Ollama API URL
            model: 使用するLLMモデル名
            timeout: タイムアウト時間（秒）
            max_retries: リトライ回数
        """
    
    def generate(self, messages: List[Dict[str, str]], 
                 temperature: float = 0.7, 
                 max_tokens: int = 2000) -> str:
        """
        テキスト生成
        
        Args:
            messages: OpenAI形式のメッセージリスト
            temperature: 生成の多様性（0.0-2.0）
            max_tokens: 最大生成トークン数
        
        Returns:
            生成されたテキスト
        
        Raises:
            ConnectionError: Ollama接続失敗
            TimeoutError: タイムアウト
        """
    
    def embed(self, text: str, 
              model: str = "mxbai-embed-large") -> List[float]:
        """
        埋め込みベクトル生成
        
        Args:
            text: 埋め込み対象テキスト
            model: 埋め込みモデル名
        
        Returns:
            埋め込みベクトル（リスト）
        """
    
    def is_healthy(self) -> bool:
        """
        Ollama接続確認
        
        Returns:
            接続可能ならTrue
        """
```

### RAGEngine API

```python
class RAGEngine:
    def __init__(self, ollama_client: OllamaClient, 
                 chroma_path: str):
        """
        Args:
            ollama_client: 埋め込み生成用クライアント
            chroma_path: ChromaDB永続化パス
        """
    
    def add_knowledge(self, texts: List[str], 
                     metadatas: List[Dict[str, str]]):
        """
        知識追加
        
        Args:
            texts: テキストのリスト
            metadatas: メタデータのリスト（texts と同じ長さ）
        """
    
    def search(self, query: str, top_k: int = 3,
              filters: Optional[Dict[str, str]] = None) -> List[Dict]:
        """
        類似度検索
        
        Args:
            query: 検索クエリ
            top_k: 取得件数
            filters: メタデータフィルタ
                例: {"character": "yana"}
        
        Returns:
            検索結果のリスト:
            [
                {
                    "text": "検索結果テキスト",
                    "score": 0.85,  # 類似度スコア
                    "metadata": {...}
                },
                ...
            ]
        """
    
    def init_from_files(self, knowledge_dir: str,
                       metadata_mapping: Dict[str, Dict]):
        """
        ファイルから知識ベース初期化
        
        Args:
            knowledge_dir: 知識ファイルディレクトリ
            metadata_mapping: ファイル名→メタデータのマッピング
        """
```

### Character API

```python
class Character:
    def __init__(self, name: str, config_path: str,
                 ollama_client: OllamaClient,
                 rag_engine: RAGEngine):
        """
        Args:
            name: キャラクター名（"yana" | "ayu"）
            config_path: 設定ファイルパス
            ollama_client: LLMクライアント
            rag_engine: RAG検索エンジン
        """
    
    def respond(self, user_input: str,
               use_rag: bool = True,
               rewrite_query: bool = False) -> str:
        """
        応答生成
        
        Args:
            user_input: ユーザー入力
            use_rag: RAG検索を使用するか
            rewrite_query: クエリ書き換えを使用するか
        
        Returns:
            応答テキスト
        """
    
    def clear_history(self):
        """会話履歴をクリア"""
```

詳細な実装例と使用例は [07_実装サンプル.md](./07_実装サンプル.md) を参照してください。

---

## 状態管理

### Character の状態

```python
{
    "name": "yana",
    "history": [
        {"role": "user", "content": "..."},
        {"role": "assistant", "content": "..."},
    ],
    "turn_count": 5,
    "last_rag_search": "...",  # デバッグ用
}
```

### RAGEngine の状態

```python
{
    "chroma_initialized": True,
    "document_count": 150,
    "last_search_time": 0.12,  # 秒
}
```

---

## エラー処理戦略

### エラーの種類と対応

| エラー種別 | 検出場所 | 対応策 |
|-----------|---------|--------|
| Ollama接続失敗 | OllamaClient | リトライ3回 → エラーメッセージ |
| タイムアウト | OllamaClient | タイムアウト30秒 → 中断 |
| ChromaDB初期化失敗 | RAGEngine | ディレクトリ作成 → 再試行 |
| 知識ファイル不在 | RAGEngine | 警告ログ → 続行 |
| 設定ファイル不正 | Character | デフォルト値使用 → 警告 |

### ログレベル

```
DEBUG: 詳細なデバッグ情報
INFO: 通常動作の情報
WARNING: 問題だが継続可能
ERROR: エラーだが回復可能
CRITICAL: 致命的エラー
```

---

## パフォーマンス要件

### レスポンス時間目標

| 処理 | 目標時間 |
|------|---------|
| 初回起動（知識投入） | < 30秒 |
| 2回目以降起動 | < 5秒 |
| RAG検索 | < 0.5秒 |
| LLM生成（Gemma3-12B） | < 3秒 |
| 全体応答時間 | < 4秒 |

### リソース使用量目標

| リソース | 目標値 |
|---------|--------|
| VRAM（Gemma3-12B） | < 12GB |
| RAM | < 4GB |
| ディスク（ChromaDB） | < 500MB |

---

## セキュリティ考慮事項

### ローカル実行の利点

- APIキー不要
- データがローカルに留まる
- インターネット接続不要（モデルダウンロード後）

### 注意点

- 知識ベースに機密情報を含めない
- 生成された応答の責任は使用者にある

---

## 拡張性への配慮

### 将来の拡張ポイント

1. **複数LLMサポート**
   - OllamaClient を抽象化
   - vLLM, OpenAI API などに対応

2. **マルチモーダル対応**
   - 画像入力のサポート
   - Florence-2統合

3. **高度な検索戦略**
   - ハイブリッド検索（BM25 + ベクトル）
   - リランキング

4. **分散実行**
   - LLMとRAGを別サーバーで実行

これらはモジュール境界を明確にすることで、将来的に追加可能。

---

**作成日**: 2026年1月14日  
**最終更新**: 2026年1月14日（レイヤー名とクラス名を統一、実装コードを 07_実装サンプル.md に分離）
